[{"id":0,"href":"/docs/track1/getting-started/","title":"Getting Started with Track 1","section":"Track 1","content":"Overview# For Track 1, we\u0026rsquo;ve allocated 2 shared vLLM instances to be used for your solutions in the challenge. These instances are OpenAI API compatible, meaning they can be interacted with tooling and libraries originally created for working with OpenAI.\nThe first instance can be located at eidf219-main.vms.os.eidf.epcc.ed.ac.uk:8000 and is a Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8 model under the hood, to be used for general purpose queries and code generation. The second instance is a visual reasoning focused model and can be found at eidf219-main.vms.os.eidf.epcc.ed.ac.uk:8001 (a Qwen/Qwen2.5-VL-32B-Instruct model under the hood).\nStep 1: Connecting to the API# Connecting from your team\u0026rsquo;s VM# The first and easiest way to connect to the API is to connect to your team\u0026rsquo;s VM by following the instructions found here. From there, you can run a test query against the vLLM instances like so:\ncurl http://eidf219-main.vms.os.eidf.epcc.ed.ac.uk:8000/v1/chat/completions -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Give me a short introduction to large language models.\u0026#34;} ], \u0026#34;temperature\u0026#34;: 0.6, \u0026#34;top_p\u0026#34;: 0.95, \u0026#34;top_k\u0026#34;: 20, \u0026#34;max_tokens\u0026#34;: 32768 }\u0026#39;Connecting from your local machine with SSH tunelling# To connect to the vLLM instances from your local machine, you can use SSH tunneling. In a standard bash shell, the command looks like so,\nssh -J \u0026lt;your_username\u0026gt;@eidf-gateway.epcc.ed.ac.uk -L 8000:10.1.0.155:8000 -N \u0026lt;your_username\u0026gt;@\u0026lt;vm_ip\u0026gt;where \u0026lt;your_username\u0026gt; is replaced with the username given for your team\u0026rsquo;s VM, and \u0026lt;vm_ip\u0026gt; is the EIDF IP address associated with your team\u0026rsquo;s VM. Note: when connecting to the visual reasoning vLLM instance (the one using a Qwen/Qwen2.5-VL-32B-Instruct mode), replace 8000:10.1.0.155:8000 with 8001:10.1.0.155:8001.\nOnce you\u0026rsquo;ve done this, in a new window you should be able to run a similar test, now by querying the API as localhost:8000 (or localhost:8001 if you\u0026rsquo;re querying the visual reasoning instance).\ncurl http://localhost:8000/v1/chat/completions -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\u0026#34;, \u0026#34;messages\u0026#34;: [ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Give me a short introduction to large language models.\u0026#34;} ], \u0026#34;temperature\u0026#34;: 0.6, \u0026#34;top_p\u0026#34;: 0.95, \u0026#34;top_k\u0026#34;: 20, \u0026#34;max_tokens\u0026#34;: 32768 }\u0026#39;Step 2: Programmatically interacting with the API# In Python, the openai package can be used to interact with the vLLM instance. Under the hood, this is largely a wrapper around the OpenAI REST API, so the insights from the previous sections still apply. I\u0026rsquo;ve included a few code samples to get you started with these below.\nTo query the Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8 model using the openai package:\nfrom openai import OpenAI openai_api_key = \u0026#34;EMPTY\u0026#34; # This can be left alone # Uncomment depending on where you\u0026#39;re running this script: # openai_api_base = \u0026#34;http://localhost:8000/v1\u0026#34; # I\u0026#39;m running from my local machine with SSH tunneling # openai_api_base = \u0026#34;http://eidf219-main.vms.os.eidf.epcc.ed.ac.uk:8000/v1\u0026#34; # I\u0026#39;m running from my team\u0026#39;s VM client = OpenAI( api_key=openai_api_key, base_url=openai_api_base, ) chat_response = client.chat.completions.create( model=\u0026#34;Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Give me a short introduction to large language models.\u0026#34;}, ], max_tokens=32768, temperature=0.6, top_p=0.95, extra_body={ \u0026#34;top_k\u0026#34;: 20, }, ) print(\u0026#34;Chat response:\u0026#34;, chat_response)To query the Qwen/Qwen2.5-VL-32B-Instruct model using REST API requests:\nimport requests import base64 # Uncomment depending on where you\u0026#39;re running this script: # openai_api_base = \u0026#34;http://localhost:8001/v1\u0026#34; # I\u0026#39;m running from my local machine with SSH tunneling # openai_api_base = \u0026#34;http://eidf219-main.vms.os.eidf.epcc.ed.ac.uk:8001/v1\u0026#34; # I\u0026#39;m running from my team\u0026#39;s VM # Replace this with your actual image URL image_url = \u0026#34;https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\u0026#34; # Download the image and encode it as base64 image_data = requests.get(image_url).content image_base64 = base64.b64encode(image_data).decode(\u0026#39;utf-8\u0026#39;) # Prepare the payload for the OpenAI-compatible API payload = { \u0026#34;model\u0026#34;: \u0026#34;Qwen/Qwen2.5-VL-32B-Instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;type\u0026#34;: \u0026#34;image_url\u0026#34;, \u0026#34;image_url\u0026#34;: {\u0026#34;url\u0026#34;: f\u0026#34;data:image/jpeg;base64,{image_base64}\u0026#34;}}, {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;What is inside this image?\u0026#34;} ] } ] } # Send the request to the local server response = requests.post(f\u0026#34;{openai_api_base}/chat/completions\u0026#34;, json=payload) # Print the model\u0026#39;s response print(response.json())"},{"id":1,"href":"/docs/track2/getting-started/","title":"Getting Started with Track 2","section":"Track 2","content":"Overview# This document will include challenge specific information and is useful to read even if you have prior experience with Kubernetes.\nTrack 2 teams will be given access to a shared kubernetes compute cluster. This cluster makes available 6 nodes, 4 of which with 8 x H200 GPUs, and 2 of which with 8 x H100 GPUs. The cluster is to be used for training and tuning your models.\nNote that the cluster can only be interacted with through your team\u0026rsquo;s VM. So, all commands given should be run on your team\u0026rsquo;s VM.\nCrash Course Kubernetes# In Kubernetes, state is expressed in the form of resources. Some common resource types are pods (like containers in docker), deployments (similar to a docker-compose file), and jobs.\nYou can perform actions on resources using the kubectl utility. For instance,\n# To list pods running: kubectl get pods # To create a resource from a file: kubectl -n eidf219ns create -f myjob.yaml # To see the logs emitted from a pod (pods are spawned by jobs) kubectl -n eidf219ns logs \u0026lt;pod_name\u0026gt; # To delete a broken or finished job: kubectl -n eidf219ns delete job \u0026lt;job_name\u0026gt;For this challenge, Job resources are the most relevent.\nRunning your first job# Jobs can be specified with yaml files. This makes them reproducible and much easier to debug. To actually create a resource specified by a yaml file, you can use the command kubectl -n eidf219ns create -f /path/to/file.yaml.\nAs an example:\napiVersion: batch/v1 kind: Job metadata: generateName: jobtest- labels: kueue.x-k8s.io/queue-name: eidf219ns-user-queue spec: completions: 1 backoffLimit: 1 ttlSecondsAfterFinished: 1800 template: metadata: name: job-test spec: containers: - name: cudasample image: nvcr.io/nvidia/k8s/cuda-sample:nbody-cuda11.7.1 args: [\u0026#34;-benchmark\u0026#34;, \u0026#34;-numbodies=512000\u0026#34;, \u0026#34;-fp64\u0026#34;, \u0026#34;-fullscreen\u0026#34;] resources: requests: cpu: 2 memory: \u0026#39;1Gi\u0026#39; limits: cpu: 2 memory: \u0026#39;4Gi\u0026#39; nvidia.com/gpu: 1 restartPolicy: NeverClaiming GPUs# The following will attempt to claim a GPU and then display PCIE devices in the logs.\napiVersion: batch/v1 kind: Job metadata: generateName: gpu-test-1- labels: kueue.x-k8s.io/queue-name: eidf219ns-user-queue app: gputest1 spec: # Selecter would go here template: metadata: labels: app: gputest1 spec: restartPolicy: Never containers: - name: ubuntu image: ubuntu:20.04 command: [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;apt-get update \u0026amp;\u0026amp; apt-get install lshw -y \u0026amp;\u0026amp; lshw -C display\u0026#34;] resources: limits: nvidia.com/gpu: 1 cpu: 1 memory: \u0026#34;4Gi\u0026#34;By default, it will pick a GPU variant and attempt to select specifically nodes with that GPU, so if you see that your Job is marked as pending for long periods, it may be worth trying to select other types of GPUs. This can be done adding the following lines where it says \u0026ldquo;Selecter would go here\u0026rdquo; in the code sample above:\nnodeSelector: nvidia.com/gpu-product: \u0026#39;\u0026lt;GPU Type\u0026gt;\u0026#39;See the EIDF Doc\u0026rsquo;s for more information.\nFAQ / Common Issues# Setting up my default namespace# If you\u0026rsquo;d like to avoid having to type -n eidf219ns every time, you can follow the instructions here to setup your default namespace.\nDoing so allows you to simply type kubectl get pods for instance to list all pods, rather than kubectl -n eidf219ns get pods\nPersistent Volume Claims# If you\u0026rsquo;d like a PVC created for your team, please message @emily.747 on Discord or email me.\nFurther Documentation# I\u0026rsquo;d highly recommend the following resources and documentation when you\u0026rsquo;re stuck:\nEIDF\u0026rsquo;s GPU Service Documentation Kubernetes Docs "},{"id":2,"href":"/showcases/","title":"Showcases","section":"","content":" At the moment this page is empty# hi\n"}]